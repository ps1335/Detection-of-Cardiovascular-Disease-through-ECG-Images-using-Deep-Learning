{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adf96a0-1c2c-4c15-b11c-e4925d1be5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from natsort import natsorted\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a940f6eb-a9d9-492c-9fe5-03b4c21f358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list to store file_names\n",
    "NORMAL_=[]\n",
    "MI_=[]\n",
    "PMI_=[]\n",
    "HB_=[]\n",
    "\n",
    "normal = 'E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\preprocessed_1d\\\\NORMAL'\n",
    "abnormal = 'E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\preprocessed_1d\\\\HB'\n",
    "MI = 'E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\preprocessed_1d\\\\MI'\n",
    "MI_history = 'E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\preprocessed_1d\\\\PMI'\n",
    "\n",
    "Types_ECG = {'normal':normal,'Abnormal_hear_beat':abnormal,'MI':MI,'History_MI':MI_history}\n",
    "\n",
    "for types,folder in Types_ECG.items():\n",
    "  for files in os.listdir(folder):\n",
    "    if types=='normal':\n",
    "      NORMAL_.append(files)\n",
    "    elif types=='Abnormal_hear_beat':\n",
    "      HB_.append(files)\n",
    "    elif types=='MI':\n",
    "      MI_.append(files)\n",
    "    elif types=='History_MI':\n",
    "      PMI_.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0eb2d59-3c69-4288-b012-5f257f862215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORMAL_ = natsorted(NORMAL_)\n",
    "NORMAL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29ed3d4-e357-46d5-8eb4-e36098b233c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_ = natsorted(MI_)\n",
    "MI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286a2b5d-8653-4286-b711-003102d02bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMI_ = natsorted(PMI_)\n",
    "PMI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ded123f-5e9b-4849-a9d6-369d11ce261a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HB_ = natsorted(HB_)\n",
    "HB_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9626fd-673d-4262-85aa-cd5001205508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over and create combined csv files for each leads.\n",
    "for x in range(len(MI_)):\n",
    "  df1=pd.read_csv('E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\preprocessed_1d\\\\NORMAL\\\\{}'.format(NORMAL_[x]))\n",
    "  df2=pd.read_csv('E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\preprocessed_1d\\\\HB\\\\{}'.format(HB_[x]))\n",
    "  df3=pd.read_csv('E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\preprocessed_1d\\\\MI\\\\{}'.format(MI_[x]))\n",
    "  df4=pd.read_csv('E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\preprocessed_1d\\\\PMI\\\\{}'.format(PMI_[x]))\n",
    "  final_df = pd.concat([df1,df2,df3,df4],ignore_index=True)\n",
    "  final_df.to_csv('E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\CombinedId_csv'.format(x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a83939ca-0c09-4f88-8dd0-d75011bd00ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'HB', 'MI', 'PM'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now reading just lead1\n",
    "df=pd.read_csv('E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\Combined1d_csv\\\\Combined_IDLead_1.csv')\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "150ac9a8-9bc5-4a03-a7dd-182199c5cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b0e6fc-940b-4010-aa73-387bf74a9d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963402</td>\n",
       "      <td>0.966190</td>\n",
       "      <td>0.929274</td>\n",
       "      <td>0.865036</td>\n",
       "      <td>0.794078</td>\n",
       "      <td>0.704745</td>\n",
       "      <td>0.611444</td>\n",
       "      <td>0.513448</td>\n",
       "      <td>0.423232</td>\n",
       "      <td>0.368119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309006</td>\n",
       "      <td>0.400056</td>\n",
       "      <td>0.495693</td>\n",
       "      <td>0.592619</td>\n",
       "      <td>0.682971</td>\n",
       "      <td>0.780323</td>\n",
       "      <td>0.865936</td>\n",
       "      <td>0.918180</td>\n",
       "      <td>0.925146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.541375</td>\n",
       "      <td>0.581143</td>\n",
       "      <td>0.620171</td>\n",
       "      <td>0.642222</td>\n",
       "      <td>0.669337</td>\n",
       "      <td>0.689173</td>\n",
       "      <td>0.766609</td>\n",
       "      <td>0.876878</td>\n",
       "      <td>0.942333</td>\n",
       "      <td>0.848699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481424</td>\n",
       "      <td>0.635611</td>\n",
       "      <td>0.722129</td>\n",
       "      <td>0.642143</td>\n",
       "      <td>0.584403</td>\n",
       "      <td>0.547887</td>\n",
       "      <td>0.505982</td>\n",
       "      <td>0.458419</td>\n",
       "      <td>0.412658</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933871</td>\n",
       "      <td>0.933860</td>\n",
       "      <td>0.929573</td>\n",
       "      <td>0.902375</td>\n",
       "      <td>0.902041</td>\n",
       "      <td>0.869612</td>\n",
       "      <td>0.828913</td>\n",
       "      <td>0.814614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731199</td>\n",
       "      <td>0.777451</td>\n",
       "      <td>0.812820</td>\n",
       "      <td>0.828121</td>\n",
       "      <td>0.843195</td>\n",
       "      <td>0.833822</td>\n",
       "      <td>0.844328</td>\n",
       "      <td>0.844897</td>\n",
       "      <td>0.844897</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.857754</td>\n",
       "      <td>0.848815</td>\n",
       "      <td>0.819930</td>\n",
       "      <td>0.760774</td>\n",
       "      <td>0.691579</td>\n",
       "      <td>0.611855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566292</td>\n",
       "      <td>0.646234</td>\n",
       "      <td>0.723609</td>\n",
       "      <td>0.784745</td>\n",
       "      <td>0.810104</td>\n",
       "      <td>0.817961</td>\n",
       "      <td>0.817854</td>\n",
       "      <td>0.809803</td>\n",
       "      <td>0.799044</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.894249</td>\n",
       "      <td>0.799704</td>\n",
       "      <td>0.674077</td>\n",
       "      <td>0.664225</td>\n",
       "      <td>0.775689</td>\n",
       "      <td>0.861098</td>\n",
       "      <td>0.877680</td>\n",
       "      <td>0.876781</td>\n",
       "      <td>0.853368</td>\n",
       "      <td>0.834667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765732</td>\n",
       "      <td>0.659982</td>\n",
       "      <td>0.543587</td>\n",
       "      <td>0.413399</td>\n",
       "      <td>0.296027</td>\n",
       "      <td>0.168565</td>\n",
       "      <td>0.067618</td>\n",
       "      <td>0.082788</td>\n",
       "      <td>0.191967</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429721</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.642137</td>\n",
       "      <td>0.742063</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.777622</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.759294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.821040</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.802413</td>\n",
       "      <td>0.808188</td>\n",
       "      <td>0.809180</td>\n",
       "      <td>0.796091</td>\n",
       "      <td>0.779687</td>\n",
       "      <td>0.778686</td>\n",
       "      <td>0.777388</td>\n",
       "      <td>0.796759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733465</td>\n",
       "      <td>0.725209</td>\n",
       "      <td>0.729679</td>\n",
       "      <td>0.748875</td>\n",
       "      <td>0.758765</td>\n",
       "      <td>0.759886</td>\n",
       "      <td>0.757626</td>\n",
       "      <td>0.748524</td>\n",
       "      <td>0.759931</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.834846</td>\n",
       "      <td>0.835026</td>\n",
       "      <td>0.848415</td>\n",
       "      <td>0.850183</td>\n",
       "      <td>0.852194</td>\n",
       "      <td>0.850544</td>\n",
       "      <td>0.852060</td>\n",
       "      <td>0.881185</td>\n",
       "      <td>0.913712</td>\n",
       "      <td>0.940417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775646</td>\n",
       "      <td>0.731681</td>\n",
       "      <td>0.723865</td>\n",
       "      <td>0.718354</td>\n",
       "      <td>0.728084</td>\n",
       "      <td>0.726277</td>\n",
       "      <td>0.690386</td>\n",
       "      <td>0.701059</td>\n",
       "      <td>0.699635</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.793255</td>\n",
       "      <td>0.703758</td>\n",
       "      <td>0.586116</td>\n",
       "      <td>0.458812</td>\n",
       "      <td>0.332085</td>\n",
       "      <td>0.216252</td>\n",
       "      <td>0.189161</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.399837</td>\n",
       "      <td>0.506746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235414</td>\n",
       "      <td>0.122761</td>\n",
       "      <td>0.045529</td>\n",
       "      <td>0.033899</td>\n",
       "      <td>0.136884</td>\n",
       "      <td>0.253815</td>\n",
       "      <td>0.371475</td>\n",
       "      <td>0.487262</td>\n",
       "      <td>0.601039</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.929683</td>\n",
       "      <td>0.934702</td>\n",
       "      <td>0.922929</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.770322</td>\n",
       "      <td>0.674787</td>\n",
       "      <td>0.572909</td>\n",
       "      <td>0.491908</td>\n",
       "      <td>0.405543</td>\n",
       "      <td>0.378534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287430</td>\n",
       "      <td>0.395687</td>\n",
       "      <td>0.503943</td>\n",
       "      <td>0.610437</td>\n",
       "      <td>0.706004</td>\n",
       "      <td>0.800673</td>\n",
       "      <td>0.854129</td>\n",
       "      <td>0.867868</td>\n",
       "      <td>0.841508</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.963402  0.966190  0.929274  0.865036  0.794078  0.704745  0.611444   \n",
       "1    0.541375  0.581143  0.620171  0.642222  0.669337  0.689173  0.766609   \n",
       "2    0.933871  0.933871  0.933871  0.933860  0.929573  0.902375  0.902041   \n",
       "3    0.840359  0.847049  0.852590  0.860626  0.857754  0.848815  0.819930   \n",
       "4    0.894249  0.799704  0.674077  0.664225  0.775689  0.861098  0.877680   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
       "924  0.821040  0.803252  0.802413  0.808188  0.809180  0.796091  0.779687   \n",
       "925  0.834846  0.835026  0.848415  0.850183  0.852194  0.850544  0.852060   \n",
       "926  0.793255  0.703758  0.586116  0.458812  0.332085  0.216252  0.189161   \n",
       "927  0.929683  0.934702  0.922929  0.858824  0.770322  0.674787  0.572909   \n",
       "\n",
       "            7         8         9  ...       246       247       248  \\\n",
       "0    0.513448  0.423232  0.368119  ...  0.309006  0.400056  0.495693   \n",
       "1    0.876878  0.942333  0.848699  ...  0.481424  0.635611  0.722129   \n",
       "2    0.869612  0.828913  0.814614  ...  0.731199  0.777451  0.812820   \n",
       "3    0.760774  0.691579  0.611855  ...  0.566292  0.646234  0.723609   \n",
       "4    0.876781  0.853368  0.834667  ...  0.765732  0.659982  0.543587   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.821865  0.721302  0.612039  ...  0.429721  0.531567  0.642137   \n",
       "924  0.778686  0.777388  0.796759  ...  0.733465  0.725209  0.729679   \n",
       "925  0.881185  0.913712  0.940417  ...  0.775646  0.731681  0.723865   \n",
       "926  0.293799  0.399837  0.506746  ...  0.235414  0.122761  0.045529   \n",
       "927  0.491908  0.405543  0.378534  ...  0.287430  0.395687  0.503943   \n",
       "\n",
       "          249       250       251       252       253       254  target  \n",
       "0    0.592619  0.682971  0.780323  0.865936  0.918180  0.925146       2  \n",
       "1    0.642143  0.584403  0.547887  0.505982  0.458419  0.412658       2  \n",
       "2    0.828121  0.843195  0.833822  0.844328  0.844897  0.844897       2  \n",
       "3    0.784745  0.810104  0.817961  0.817854  0.809803  0.799044       2  \n",
       "4    0.413399  0.296027  0.168565  0.067618  0.082788  0.191967       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.742063  0.833042  0.814867  0.777622  0.760714  0.759294       3  \n",
       "924  0.748875  0.758765  0.759886  0.757626  0.748524  0.759931       3  \n",
       "925  0.718354  0.728084  0.726277  0.690386  0.701059  0.699635       3  \n",
       "926  0.033899  0.136884  0.253815  0.371475  0.487262  0.601039       3  \n",
       "927  0.610437  0.706004  0.800673  0.854129  0.867868  0.841508       3  \n",
       "\n",
       "[928 rows x 256 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert Target column values as Numeric using ngroups\n",
    "encode_target_label = df.groupby('Target').ngroup().rename(\"target\").to_frame()\n",
    "test_final  = df.merge(encode_target_label, left_index=True, right_index=True)\n",
    "test_final.drop(columns=['Target'],inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c4062-4ae2-4e5a-bd7c-7a26f9778331",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFORM DIMENSIONALITY REDUCTION JUST FOR CHECKING/UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f62caaf-9868-4305-967f-9a982d21a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [1.76631711e-01 9.47841846e-02 7.00100519e-02 6.14026654e-02\n",
      " 5.33383989e-02 4.23623127e-02 3.69098118e-02 3.38745152e-02\n",
      " 3.00158981e-02 2.89945688e-02 2.64691913e-02 2.42569640e-02\n",
      " 2.11690566e-02 1.99633083e-02 1.77748538e-02 1.62162835e-02\n",
      " 1.52721200e-02 1.48309040e-02 1.34418063e-02 1.20062516e-02\n",
      " 1.17530896e-02 1.05531342e-02 9.63050847e-03 9.41801238e-03\n",
      " 8.64223009e-03 8.47025185e-03 7.91878283e-03 7.30862520e-03\n",
      " 6.84760694e-03 6.38087529e-03 5.98449751e-03 5.47038359e-03\n",
      " 5.29039352e-03 4.97484057e-03 4.76542228e-03 4.46934342e-03\n",
      " 4.23682780e-03 4.00132225e-03 3.82983831e-03 3.52568125e-03\n",
      " 3.38250175e-03 3.25792933e-03 3.08591094e-03 3.01371361e-03\n",
      " 2.73110840e-03 2.50310601e-03 2.36348041e-03 2.27449529e-03\n",
      " 2.18960064e-03 1.99106312e-03 1.73887639e-03 1.70320722e-03\n",
      " 1.57079343e-03 1.50915169e-03 1.38122660e-03 1.32580398e-03\n",
      " 1.26115946e-03 1.18773619e-03 1.17785245e-03 1.09298665e-03\n",
      " 1.02856003e-03 9.21386903e-04 7.92491113e-04 7.71182408e-04\n",
      " 6.49755057e-04 6.22135582e-04 6.05545238e-04 5.49402227e-04\n",
      " 5.33573429e-04 4.83661469e-04 4.74732124e-04 4.49183915e-04\n",
      " 4.32753527e-04 4.09160745e-04 3.95604745e-04 3.71414717e-04\n",
      " 3.55188513e-04 3.37060857e-04 3.05051417e-04 2.96587598e-04\n",
      " 2.79140203e-04 2.64923159e-04 2.45988818e-04 2.37436077e-04\n",
      " 2.13299822e-04 1.99891379e-04 1.95899152e-04 1.86585802e-04\n",
      " 1.66966012e-04 1.62157807e-04 1.56788783e-04 1.40734966e-04\n",
      " 1.24999211e-04 1.21327550e-04 1.13841355e-04 1.09516045e-04\n",
      " 1.02587538e-04 9.95528762e-05 9.71458647e-05 9.10686448e-05]\n",
      "\n",
      " Total Variance Explained: 99.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.349042</td>\n",
       "      <td>-2.463929</td>\n",
       "      <td>1.349728</td>\n",
       "      <td>-0.274637</td>\n",
       "      <td>0.172031</td>\n",
       "      <td>-0.152916</td>\n",
       "      <td>0.821459</td>\n",
       "      <td>-0.098088</td>\n",
       "      <td>-1.356674</td>\n",
       "      <td>-0.221998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055581</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.039549</td>\n",
       "      <td>-0.011546</td>\n",
       "      <td>0.013282</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.020048</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.033220</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.082457</td>\n",
       "      <td>0.419848</td>\n",
       "      <td>-0.373862</td>\n",
       "      <td>-0.017678</td>\n",
       "      <td>-0.475013</td>\n",
       "      <td>0.472410</td>\n",
       "      <td>-0.089024</td>\n",
       "      <td>0.241118</td>\n",
       "      <td>0.080011</td>\n",
       "      <td>0.072828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020374</td>\n",
       "      <td>0.068266</td>\n",
       "      <td>-0.020873</td>\n",
       "      <td>-0.006832</td>\n",
       "      <td>0.068137</td>\n",
       "      <td>-0.018867</td>\n",
       "      <td>-0.076820</td>\n",
       "      <td>0.035075</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.228324</td>\n",
       "      <td>1.172645</td>\n",
       "      <td>-0.524623</td>\n",
       "      <td>0.303817</td>\n",
       "      <td>-0.000940</td>\n",
       "      <td>0.554329</td>\n",
       "      <td>0.218682</td>\n",
       "      <td>-0.507440</td>\n",
       "      <td>-0.524708</td>\n",
       "      <td>-1.222993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017976</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.036630</td>\n",
       "      <td>-0.030204</td>\n",
       "      <td>0.015947</td>\n",
       "      <td>-0.050829</td>\n",
       "      <td>-0.019111</td>\n",
       "      <td>-0.031944</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.852269</td>\n",
       "      <td>0.621011</td>\n",
       "      <td>0.801915</td>\n",
       "      <td>1.339401</td>\n",
       "      <td>-0.849406</td>\n",
       "      <td>-1.500549</td>\n",
       "      <td>1.074489</td>\n",
       "      <td>1.071347</td>\n",
       "      <td>0.307250</td>\n",
       "      <td>0.134665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018945</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>-0.022334</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.048733</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.038220</td>\n",
       "      <td>-1.476702</td>\n",
       "      <td>-0.942818</td>\n",
       "      <td>-0.519393</td>\n",
       "      <td>0.313444</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>-0.505386</td>\n",
       "      <td>-0.626590</td>\n",
       "      <td>0.189309</td>\n",
       "      <td>-0.812674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043976</td>\n",
       "      <td>-0.089271</td>\n",
       "      <td>-0.004694</td>\n",
       "      <td>-0.051876</td>\n",
       "      <td>-0.031203</td>\n",
       "      <td>0.032577</td>\n",
       "      <td>0.033667</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>-0.004594</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-0.865127</td>\n",
       "      <td>-0.046079</td>\n",
       "      <td>0.937729</td>\n",
       "      <td>0.310634</td>\n",
       "      <td>-0.456238</td>\n",
       "      <td>-0.378867</td>\n",
       "      <td>1.068905</td>\n",
       "      <td>0.782661</td>\n",
       "      <td>0.659660</td>\n",
       "      <td>1.011713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026413</td>\n",
       "      <td>-0.103142</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.008699</td>\n",
       "      <td>-0.022401</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>-0.040757</td>\n",
       "      <td>0.045906</td>\n",
       "      <td>0.013152</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.647144</td>\n",
       "      <td>1.731302</td>\n",
       "      <td>0.737931</td>\n",
       "      <td>-0.076732</td>\n",
       "      <td>0.066095</td>\n",
       "      <td>-0.406027</td>\n",
       "      <td>-0.302324</td>\n",
       "      <td>0.081770</td>\n",
       "      <td>-0.083183</td>\n",
       "      <td>-1.064898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>-0.011316</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>-0.037920</td>\n",
       "      <td>-0.004110</td>\n",
       "      <td>-0.008658</td>\n",
       "      <td>-0.025495</td>\n",
       "      <td>-0.021845</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>-1.102371</td>\n",
       "      <td>1.278477</td>\n",
       "      <td>-0.331359</td>\n",
       "      <td>-0.922451</td>\n",
       "      <td>0.158848</td>\n",
       "      <td>0.131820</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>-0.547190</td>\n",
       "      <td>0.617257</td>\n",
       "      <td>-0.391317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065837</td>\n",
       "      <td>-0.012618</td>\n",
       "      <td>0.060469</td>\n",
       "      <td>-0.013656</td>\n",
       "      <td>-0.076074</td>\n",
       "      <td>-0.034352</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>-0.030425</td>\n",
       "      <td>0.009358</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>-0.276794</td>\n",
       "      <td>-0.306218</td>\n",
       "      <td>1.012962</td>\n",
       "      <td>-0.827206</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>1.570077</td>\n",
       "      <td>0.954821</td>\n",
       "      <td>-0.816475</td>\n",
       "      <td>-0.624750</td>\n",
       "      <td>1.503205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005076</td>\n",
       "      <td>0.028586</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.042793</td>\n",
       "      <td>0.056169</td>\n",
       "      <td>0.041837</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>-0.021081</td>\n",
       "      <td>-0.020459</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.631759</td>\n",
       "      <td>0.586710</td>\n",
       "      <td>1.745229</td>\n",
       "      <td>-0.184511</td>\n",
       "      <td>-1.256110</td>\n",
       "      <td>0.661976</td>\n",
       "      <td>0.903205</td>\n",
       "      <td>0.185535</td>\n",
       "      <td>0.257371</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.071040</td>\n",
       "      <td>-0.010823</td>\n",
       "      <td>-0.031883</td>\n",
       "      <td>-0.024223</td>\n",
       "      <td>-0.019152</td>\n",
       "      <td>0.028242</td>\n",
       "      <td>-0.008276</td>\n",
       "      <td>0.022432</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.349042 -2.463929  1.349728 -0.274637  0.172031 -0.152916  0.821459   \n",
       "1    2.082457  0.419848 -0.373862 -0.017678 -0.475013  0.472410 -0.089024   \n",
       "2   -2.228324  1.172645 -0.524623  0.303817 -0.000940  0.554329  0.218682   \n",
       "3   -0.852269  0.621011  0.801915  1.339401 -0.849406 -1.500549  1.074489   \n",
       "4   -0.038220 -1.476702 -0.942818 -0.519393  0.313444  0.009207 -0.505386   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -0.865127 -0.046079  0.937729  0.310634 -0.456238 -0.378867  1.068905   \n",
       "924 -0.647144  1.731302  0.737931 -0.076732  0.066095 -0.406027 -0.302324   \n",
       "925 -1.102371  1.278477 -0.331359 -0.922451  0.158848  0.131820  0.004359   \n",
       "926 -0.276794 -0.306218  1.012962 -0.827206  0.380327  1.570077  0.954821   \n",
       "927 -1.631759  0.586710  1.745229 -0.184511 -1.256110  0.661976  0.903205   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0   -0.098088 -1.356674 -0.221998  ... -0.055581  0.000221  0.039549   \n",
       "1    0.241118  0.080011  0.072828  ... -0.020374  0.068266 -0.020873   \n",
       "2   -0.507440 -0.524708 -1.222993  ... -0.017976  0.048912  0.036630   \n",
       "3    1.071347  0.307250  0.134665  ... -0.018945  0.006468 -0.022334   \n",
       "4   -0.626590  0.189309 -0.812674  ...  0.043976 -0.089271 -0.004694   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.782661  0.659660  1.011713  ...  0.026413 -0.103142 -0.029886   \n",
       "924  0.081770 -0.083183 -1.064898  ...  0.013290 -0.011316  0.016652   \n",
       "925 -0.547190  0.617257 -0.391317  ...  0.065837 -0.012618  0.060469   \n",
       "926 -0.816475 -0.624750  1.503205  ... -0.005076  0.028586  0.010729   \n",
       "927  0.185535  0.257371  0.549247  ...  0.019015  0.071040 -0.010823   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0   -0.011546  0.013282  0.020086  0.020048  0.032026  0.033220       2  \n",
       "1   -0.006832  0.068137 -0.018867 -0.076820  0.035075  0.027514       2  \n",
       "2   -0.030204  0.015947 -0.050829 -0.019111 -0.031944  0.012889       2  \n",
       "3    0.004478  0.010086  0.016337  0.048733  0.010628  0.020476       2  \n",
       "4   -0.051876 -0.031203  0.032577  0.033667  0.010813 -0.004594       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.008699 -0.022401  0.040246 -0.040757  0.045906  0.013152       3  \n",
       "924 -0.037920 -0.004110 -0.008658 -0.025495 -0.021845  0.009138       3  \n",
       "925 -0.013656 -0.076074 -0.034352 -0.023846 -0.030425  0.009358       3  \n",
       "926  0.042793  0.056169  0.041837  0.015151 -0.021081 -0.020459       3  \n",
       "927 -0.031883 -0.024223 -0.019152  0.028242 -0.008276  0.022432       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just for testing\n",
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 100\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(test_final.iloc[:,0:-1])\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(test_final['target'], name='target')\n",
    "result_df = pd.concat([pca_df, target], axis=1)\n",
    "result_df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9905200-9c6d-4b1f-a341-89e9fbab5298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.349042</td>\n",
       "      <td>-2.463929</td>\n",
       "      <td>1.349728</td>\n",
       "      <td>-0.274637</td>\n",
       "      <td>0.172031</td>\n",
       "      <td>-0.152916</td>\n",
       "      <td>0.821459</td>\n",
       "      <td>-0.098088</td>\n",
       "      <td>-1.356674</td>\n",
       "      <td>-0.221998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055581</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.039549</td>\n",
       "      <td>-0.011546</td>\n",
       "      <td>0.013282</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.020048</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.033220</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.082457</td>\n",
       "      <td>0.419848</td>\n",
       "      <td>-0.373862</td>\n",
       "      <td>-0.017678</td>\n",
       "      <td>-0.475013</td>\n",
       "      <td>0.472410</td>\n",
       "      <td>-0.089024</td>\n",
       "      <td>0.241118</td>\n",
       "      <td>0.080011</td>\n",
       "      <td>0.072828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020374</td>\n",
       "      <td>0.068266</td>\n",
       "      <td>-0.020873</td>\n",
       "      <td>-0.006832</td>\n",
       "      <td>0.068137</td>\n",
       "      <td>-0.018867</td>\n",
       "      <td>-0.076820</td>\n",
       "      <td>0.035075</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.228324</td>\n",
       "      <td>1.172645</td>\n",
       "      <td>-0.524623</td>\n",
       "      <td>0.303817</td>\n",
       "      <td>-0.000940</td>\n",
       "      <td>0.554329</td>\n",
       "      <td>0.218682</td>\n",
       "      <td>-0.507440</td>\n",
       "      <td>-0.524708</td>\n",
       "      <td>-1.222993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017976</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.036630</td>\n",
       "      <td>-0.030204</td>\n",
       "      <td>0.015947</td>\n",
       "      <td>-0.050829</td>\n",
       "      <td>-0.019111</td>\n",
       "      <td>-0.031944</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.852269</td>\n",
       "      <td>0.621011</td>\n",
       "      <td>0.801915</td>\n",
       "      <td>1.339401</td>\n",
       "      <td>-0.849406</td>\n",
       "      <td>-1.500549</td>\n",
       "      <td>1.074489</td>\n",
       "      <td>1.071347</td>\n",
       "      <td>0.307250</td>\n",
       "      <td>0.134665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018945</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>-0.022334</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.048733</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.038220</td>\n",
       "      <td>-1.476702</td>\n",
       "      <td>-0.942818</td>\n",
       "      <td>-0.519393</td>\n",
       "      <td>0.313444</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>-0.505386</td>\n",
       "      <td>-0.626590</td>\n",
       "      <td>0.189309</td>\n",
       "      <td>-0.812674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043976</td>\n",
       "      <td>-0.089271</td>\n",
       "      <td>-0.004694</td>\n",
       "      <td>-0.051876</td>\n",
       "      <td>-0.031203</td>\n",
       "      <td>0.032577</td>\n",
       "      <td>0.033667</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>-0.004594</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-0.865127</td>\n",
       "      <td>-0.046079</td>\n",
       "      <td>0.937729</td>\n",
       "      <td>0.310634</td>\n",
       "      <td>-0.456238</td>\n",
       "      <td>-0.378867</td>\n",
       "      <td>1.068905</td>\n",
       "      <td>0.782661</td>\n",
       "      <td>0.659660</td>\n",
       "      <td>1.011713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026413</td>\n",
       "      <td>-0.103142</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.008699</td>\n",
       "      <td>-0.022401</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>-0.040757</td>\n",
       "      <td>0.045906</td>\n",
       "      <td>0.013152</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.647144</td>\n",
       "      <td>1.731302</td>\n",
       "      <td>0.737931</td>\n",
       "      <td>-0.076732</td>\n",
       "      <td>0.066095</td>\n",
       "      <td>-0.406027</td>\n",
       "      <td>-0.302324</td>\n",
       "      <td>0.081770</td>\n",
       "      <td>-0.083183</td>\n",
       "      <td>-1.064898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>-0.011316</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>-0.037920</td>\n",
       "      <td>-0.004110</td>\n",
       "      <td>-0.008658</td>\n",
       "      <td>-0.025495</td>\n",
       "      <td>-0.021845</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>-1.102371</td>\n",
       "      <td>1.278477</td>\n",
       "      <td>-0.331359</td>\n",
       "      <td>-0.922451</td>\n",
       "      <td>0.158848</td>\n",
       "      <td>0.131820</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>-0.547190</td>\n",
       "      <td>0.617257</td>\n",
       "      <td>-0.391317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065837</td>\n",
       "      <td>-0.012618</td>\n",
       "      <td>0.060469</td>\n",
       "      <td>-0.013656</td>\n",
       "      <td>-0.076074</td>\n",
       "      <td>-0.034352</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>-0.030425</td>\n",
       "      <td>0.009358</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>-0.276794</td>\n",
       "      <td>-0.306218</td>\n",
       "      <td>1.012962</td>\n",
       "      <td>-0.827206</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>1.570077</td>\n",
       "      <td>0.954821</td>\n",
       "      <td>-0.816475</td>\n",
       "      <td>-0.624750</td>\n",
       "      <td>1.503205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005076</td>\n",
       "      <td>0.028586</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.042793</td>\n",
       "      <td>0.056169</td>\n",
       "      <td>0.041837</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>-0.021081</td>\n",
       "      <td>-0.020459</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.631759</td>\n",
       "      <td>0.586710</td>\n",
       "      <td>1.745229</td>\n",
       "      <td>-0.184511</td>\n",
       "      <td>-1.256110</td>\n",
       "      <td>0.661976</td>\n",
       "      <td>0.903205</td>\n",
       "      <td>0.185535</td>\n",
       "      <td>0.257371</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.071040</td>\n",
       "      <td>-0.010823</td>\n",
       "      <td>-0.031883</td>\n",
       "      <td>-0.024223</td>\n",
       "      <td>-0.019152</td>\n",
       "      <td>0.028242</td>\n",
       "      <td>-0.008276</td>\n",
       "      <td>0.022432</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.349042 -2.463929  1.349728 -0.274637  0.172031 -0.152916  0.821459   \n",
       "1    2.082457  0.419848 -0.373862 -0.017678 -0.475013  0.472410 -0.089024   \n",
       "2   -2.228324  1.172645 -0.524623  0.303817 -0.000940  0.554329  0.218682   \n",
       "3   -0.852269  0.621011  0.801915  1.339401 -0.849406 -1.500549  1.074489   \n",
       "4   -0.038220 -1.476702 -0.942818 -0.519393  0.313444  0.009207 -0.505386   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -0.865127 -0.046079  0.937729  0.310634 -0.456238 -0.378867  1.068905   \n",
       "924 -0.647144  1.731302  0.737931 -0.076732  0.066095 -0.406027 -0.302324   \n",
       "925 -1.102371  1.278477 -0.331359 -0.922451  0.158848  0.131820  0.004359   \n",
       "926 -0.276794 -0.306218  1.012962 -0.827206  0.380327  1.570077  0.954821   \n",
       "927 -1.631759  0.586710  1.745229 -0.184511 -1.256110  0.661976  0.903205   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0   -0.098088 -1.356674 -0.221998  ... -0.055581  0.000221  0.039549   \n",
       "1    0.241118  0.080011  0.072828  ... -0.020374  0.068266 -0.020873   \n",
       "2   -0.507440 -0.524708 -1.222993  ... -0.017976  0.048912  0.036630   \n",
       "3    1.071347  0.307250  0.134665  ... -0.018945  0.006468 -0.022334   \n",
       "4   -0.626590  0.189309 -0.812674  ...  0.043976 -0.089271 -0.004694   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.782661  0.659660  1.011713  ...  0.026413 -0.103142 -0.029886   \n",
       "924  0.081770 -0.083183 -1.064898  ...  0.013290 -0.011316  0.016652   \n",
       "925 -0.547190  0.617257 -0.391317  ...  0.065837 -0.012618  0.060469   \n",
       "926 -0.816475 -0.624750  1.503205  ... -0.005076  0.028586  0.010729   \n",
       "927  0.185535  0.257371  0.549247  ...  0.019015  0.071040 -0.010823   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0   -0.011546  0.013282  0.020086  0.020048  0.032026  0.033220       2  \n",
       "1   -0.006832  0.068137 -0.018867 -0.076820  0.035075  0.027514       2  \n",
       "2   -0.030204  0.015947 -0.050829 -0.019111 -0.031944  0.012889       2  \n",
       "3    0.004478  0.010086  0.016337  0.048733  0.010628  0.020476       2  \n",
       "4   -0.051876 -0.031203  0.032577  0.033667  0.010813 -0.004594       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.008699 -0.022401  0.040246 -0.040757  0.045906  0.013152       3  \n",
       "924 -0.037920 -0.004110 -0.008658 -0.025495 -0.021845  0.009138       3  \n",
       "925 -0.013656 -0.076074 -0.034352 -0.023846 -0.030425  0.009358       3  \n",
       "926  0.042793  0.056169  0.041837  0.015151 -0.021081 -0.020459       3  \n",
       "927 -0.031883 -0.024223 -0.019152  0.028242 -0.008276  0.022432       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85f9fb-a432-410e-b908-5ead892d3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRYING DIFFERENT ML MODELS ON A SINGLE LEAD(EX : 1) POST DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3d88d-ab0f-4228-855b-19cd5351ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91d2308e-9a57-47a2-886b-6905c9bbc9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7983870967741935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.69      0.75       105\n",
      "           1       0.94      1.00      0.97        94\n",
      "           2       0.82      0.77      0.79       112\n",
      "           3       0.56      0.74      0.63        61\n",
      "\n",
      "    accuracy                           0.80       372\n",
      "   macro avg       0.79      0.80      0.79       372\n",
      "weighted avg       0.81      0.80      0.80       372\n",
      "\n",
      "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
    "k_range = list(range(1, 9))\n",
    "parameters = dict(knn__n_neighbors=k_range)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "Knn_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a600360-a3c1-4ecb-9072-fbdf7a7add08",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32ff3265-59d0-4f39-a3a2-12daf8b7393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('lr', LogisticRegression())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "#parameters for gridsearchcv if we increase range of entries from 5 to higher value, we can get greater accurange\n",
    "c_space = np.logspace(-4, 4, 3)\n",
    "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#call GridSearchCV and set crossvalscore to 2\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "LR_Accuracy = cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "053e2c01-b91f-4b84-a3e4-440e55f669ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5913978494623656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.41      0.46       105\n",
      "           1       0.69      1.00      0.82        94\n",
      "           2       0.62      0.54      0.58       112\n",
      "           3       0.39      0.36      0.37        61\n",
      "\n",
      "    accuracy                           0.59       372\n",
      "   macro avg       0.56      0.58      0.56       372\n",
      "weighted avg       0.58      0.59      0.57       372\n",
      "\n",
      "Tuned Model Parameters: {'lr__C': 10000.0, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1eb40-a763-4bf6-8d2d-86b2e577754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5f1024f-5e6d-4dbf-89e4-68b5e5973b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7956989247311828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        93\n",
      "           1       1.00      1.00      1.00        99\n",
      "           2       0.97      0.54      0.69       117\n",
      "           3       1.00      0.65      0.79        63\n",
      "\n",
      "    accuracy                           0.80       372\n",
      "   macro avg       0.88      0.80      0.80       372\n",
      "weighted avg       0.88      0.80      0.80       372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[10],'SVM__gamma':[1]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68fb7fd-0ac3-4561-b2f6-dc6c2472d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOW COMBINING ALL 12 LEADS INTO A SINGLE CSV FILE AND THEN PERFROM MODEL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8c6534e-f736-479d-ad23-c531b1f44d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2795</th>\n",
       "      <th>2796</th>\n",
       "      <th>2797</th>\n",
       "      <th>2798</th>\n",
       "      <th>2799</th>\n",
       "      <th>2800</th>\n",
       "      <th>2801</th>\n",
       "      <th>2802</th>\n",
       "      <th>2803</th>\n",
       "      <th>2804</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.358844</td>\n",
       "      <td>0.377652</td>\n",
       "      <td>0.389065</td>\n",
       "      <td>0.365628</td>\n",
       "      <td>0.340698</td>\n",
       "      <td>0.302705</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.228810</td>\n",
       "      <td>0.187362</td>\n",
       "      <td>0.154682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653168</td>\n",
       "      <td>0.726566</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.791812</td>\n",
       "      <td>0.771458</td>\n",
       "      <td>0.750580</td>\n",
       "      <td>0.781806</td>\n",
       "      <td>0.813771</td>\n",
       "      <td>0.822412</td>\n",
       "      <td>0.822553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009186</td>\n",
       "      <td>0.029893</td>\n",
       "      <td>0.035181</td>\n",
       "      <td>0.066365</td>\n",
       "      <td>0.124201</td>\n",
       "      <td>0.187131</td>\n",
       "      <td>0.236406</td>\n",
       "      <td>0.272308</td>\n",
       "      <td>0.266938</td>\n",
       "      <td>0.244702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676283</td>\n",
       "      <td>0.775227</td>\n",
       "      <td>0.863269</td>\n",
       "      <td>0.888242</td>\n",
       "      <td>0.868106</td>\n",
       "      <td>0.823002</td>\n",
       "      <td>0.799834</td>\n",
       "      <td>0.822866</td>\n",
       "      <td>0.833099</td>\n",
       "      <td>0.816950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.693072</td>\n",
       "      <td>0.626193</td>\n",
       "      <td>0.551423</td>\n",
       "      <td>0.476648</td>\n",
       "      <td>0.406219</td>\n",
       "      <td>0.349017</td>\n",
       "      <td>0.275173</td>\n",
       "      <td>0.201068</td>\n",
       "      <td>0.143565</td>\n",
       "      <td>0.074108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937431</td>\n",
       "      <td>0.927259</td>\n",
       "      <td>0.935873</td>\n",
       "      <td>0.935165</td>\n",
       "      <td>0.925922</td>\n",
       "      <td>0.923673</td>\n",
       "      <td>0.932826</td>\n",
       "      <td>0.940149</td>\n",
       "      <td>0.923497</td>\n",
       "      <td>0.894707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.162667</td>\n",
       "      <td>0.170588</td>\n",
       "      <td>0.154917</td>\n",
       "      <td>0.142462</td>\n",
       "      <td>0.137041</td>\n",
       "      <td>0.126293</td>\n",
       "      <td>0.124038</td>\n",
       "      <td>0.110892</td>\n",
       "      <td>0.102536</td>\n",
       "      <td>0.106102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527353</td>\n",
       "      <td>0.556848</td>\n",
       "      <td>0.635323</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.789739</td>\n",
       "      <td>0.865011</td>\n",
       "      <td>0.944071</td>\n",
       "      <td>0.994303</td>\n",
       "      <td>0.974283</td>\n",
       "      <td>0.955734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069540</td>\n",
       "      <td>0.084569</td>\n",
       "      <td>0.094581</td>\n",
       "      <td>0.148656</td>\n",
       "      <td>0.197342</td>\n",
       "      <td>0.261576</td>\n",
       "      <td>0.323840</td>\n",
       "      <td>0.373148</td>\n",
       "      <td>0.437920</td>\n",
       "      <td>0.503818</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977206</td>\n",
       "      <td>0.938962</td>\n",
       "      <td>0.903348</td>\n",
       "      <td>0.884539</td>\n",
       "      <td>0.893771</td>\n",
       "      <td>0.905483</td>\n",
       "      <td>0.916732</td>\n",
       "      <td>0.932306</td>\n",
       "      <td>0.940798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>0.130944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913404</td>\n",
       "      <td>0.968015</td>\n",
       "      <td>0.992614</td>\n",
       "      <td>0.945789</td>\n",
       "      <td>0.876660</td>\n",
       "      <td>0.808906</td>\n",
       "      <td>0.741645</td>\n",
       "      <td>0.736615</td>\n",
       "      <td>0.797729</td>\n",
       "      <td>0.855637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.206689</td>\n",
       "      <td>0.217342</td>\n",
       "      <td>0.262822</td>\n",
       "      <td>0.300850</td>\n",
       "      <td>0.326582</td>\n",
       "      <td>0.342775</td>\n",
       "      <td>0.338183</td>\n",
       "      <td>0.333632</td>\n",
       "      <td>0.312207</td>\n",
       "      <td>0.326522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918699</td>\n",
       "      <td>0.968252</td>\n",
       "      <td>0.951960</td>\n",
       "      <td>0.925386</td>\n",
       "      <td>0.894958</td>\n",
       "      <td>0.892415</td>\n",
       "      <td>0.904793</td>\n",
       "      <td>0.898045</td>\n",
       "      <td>0.890530</td>\n",
       "      <td>0.900569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.831223</td>\n",
       "      <td>0.890734</td>\n",
       "      <td>0.924835</td>\n",
       "      <td>0.921895</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.816656</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.719966</td>\n",
       "      <td>0.706928</td>\n",
       "      <td>0.689606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897772</td>\n",
       "      <td>0.913123</td>\n",
       "      <td>0.860908</td>\n",
       "      <td>0.805805</td>\n",
       "      <td>0.723781</td>\n",
       "      <td>0.651304</td>\n",
       "      <td>0.620828</td>\n",
       "      <td>0.634826</td>\n",
       "      <td>0.646613</td>\n",
       "      <td>0.682733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.687090</td>\n",
       "      <td>0.708905</td>\n",
       "      <td>0.709273</td>\n",
       "      <td>0.717499</td>\n",
       "      <td>0.741011</td>\n",
       "      <td>0.722278</td>\n",
       "      <td>0.709348</td>\n",
       "      <td>0.709286</td>\n",
       "      <td>0.718217</td>\n",
       "      <td>0.741234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480570</td>\n",
       "      <td>0.480570</td>\n",
       "      <td>0.480558</td>\n",
       "      <td>0.475925</td>\n",
       "      <td>0.462933</td>\n",
       "      <td>0.436143</td>\n",
       "      <td>0.402633</td>\n",
       "      <td>0.397611</td>\n",
       "      <td>0.377389</td>\n",
       "      <td>0.356880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.084004</td>\n",
       "      <td>0.081870</td>\n",
       "      <td>0.081870</td>\n",
       "      <td>0.081875</td>\n",
       "      <td>0.086937</td>\n",
       "      <td>0.097860</td>\n",
       "      <td>0.082659</td>\n",
       "      <td>0.098894</td>\n",
       "      <td>0.095602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894105</td>\n",
       "      <td>0.887823</td>\n",
       "      <td>0.893515</td>\n",
       "      <td>0.895696</td>\n",
       "      <td>0.895707</td>\n",
       "      <td>0.895707</td>\n",
       "      <td>0.895855</td>\n",
       "      <td>0.901186</td>\n",
       "      <td>0.913217</td>\n",
       "      <td>0.931037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 2804 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.358844  0.377652  0.389065  0.365628  0.340698  0.302705  0.268031   \n",
       "1    0.009186  0.029893  0.035181  0.066365  0.124201  0.187131  0.236406   \n",
       "2    0.693072  0.626193  0.551423  0.476648  0.406219  0.349017  0.275173   \n",
       "3    0.162667  0.170588  0.154917  0.142462  0.137041  0.126293  0.124038   \n",
       "4    0.069540  0.084569  0.094581  0.148656  0.197342  0.261576  0.323840   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.130944  0.130944  0.130944  0.130944  0.130944  0.130944  0.130944   \n",
       "924  0.206689  0.217342  0.262822  0.300850  0.326582  0.342775  0.338183   \n",
       "925  0.831223  0.890734  0.924835  0.921895  0.862000  0.816656  0.759091   \n",
       "926  0.687090  0.708905  0.709273  0.717499  0.741011  0.722278  0.709348   \n",
       "927  0.103520  0.084004  0.081870  0.081870  0.081875  0.086937  0.097860   \n",
       "\n",
       "         7         8         9     ...      2795      2796      2797  \\\n",
       "0    0.228810  0.187362  0.154682  ...  0.653168  0.726566  0.783077   \n",
       "1    0.272308  0.266938  0.244702  ...  0.676283  0.775227  0.863269   \n",
       "2    0.201068  0.143565  0.074108  ...  0.937431  0.927259  0.935873   \n",
       "3    0.110892  0.102536  0.106102  ...  0.527353  0.556848  0.635323   \n",
       "4    0.373148  0.437920  0.503818  ...  1.000000  0.977206  0.938962   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.130944  0.130944  0.130944  ...  0.913404  0.968015  0.992614   \n",
       "924  0.333632  0.312207  0.326522  ...  0.918699  0.968252  0.951960   \n",
       "925  0.719966  0.706928  0.689606  ...  0.897772  0.913123  0.860908   \n",
       "926  0.709286  0.718217  0.741234  ...  0.480570  0.480570  0.480558   \n",
       "927  0.082659  0.098894  0.095602  ...  0.894105  0.887823  0.893515   \n",
       "\n",
       "         2798      2799      2800      2801      2802      2803      2804  \n",
       "0    0.791812  0.771458  0.750580  0.781806  0.813771  0.822412  0.822553  \n",
       "1    0.888242  0.868106  0.823002  0.799834  0.822866  0.833099  0.816950  \n",
       "2    0.935165  0.925922  0.923673  0.932826  0.940149  0.923497  0.894707  \n",
       "3    0.715000  0.789739  0.865011  0.944071  0.994303  0.974283  0.955734  \n",
       "4    0.903348  0.884539  0.893771  0.905483  0.916732  0.932306  0.940798  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "923  0.945789  0.876660  0.808906  0.741645  0.736615  0.797729  0.855637  \n",
       "924  0.925386  0.894958  0.892415  0.904793  0.898045  0.890530  0.900569  \n",
       "925  0.805805  0.723781  0.651304  0.620828  0.634826  0.646613  0.682733  \n",
       "926  0.475925  0.462933  0.436143  0.402633  0.397611  0.377389  0.356880  \n",
       "927  0.895696  0.895707  0.895707  0.895855  0.901186  0.913217  0.931037  \n",
       "\n",
       "[928 rows x 2804 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "\n",
    "location = 'E:\\\\ECG DATASET\\\\ECG_DATA\\\\test\\\\Combined1d_csv\\\\'\n",
    "test_final = pd.DataFrame()  # Initialize an empty DataFrame to store the combined result\n",
    "\n",
    "# Loop through the sorted list of files\n",
    "for files in natsorted(os.listdir(location)):\n",
    "    if files.endswith(\".csv\") and not files.endswith(\"13.csv\"):\n",
    "        if files != 'Combined_IDLead_1.csv':\n",
    "            # Read the current CSV file\n",
    "            df = pd.read_csv(os.path.join(location, files))\n",
    "            \n",
    "            # Check if the column 'Unnamed: 0' exists before attempting to drop it\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            \n",
    "            # Convert all columns to numeric, forcing errors='coerce' to turn invalid values (like \"No\", \"Pm\") into NaN\n",
    "            df = df.apply(pd.to_numeric, errors='coerce')\n",
    "            \n",
    "            # Drop columns that contain any NaN values (including non-numeric values)\n",
    "            df.dropna(axis=1, inplace=True)\n",
    "            \n",
    "            # Concatenate with the existing final dataframe\n",
    "            test_final = pd.concat([test_final, df], axis=1, ignore_index=True)\n",
    "\n",
    "# Drop columns with any NaN values in the final combined dataframe (if any remain)\n",
    "test_final.dropna(axis=1, inplace=True)\n",
    "\n",
    "# Drop the target column (ensure that column 255 exists before dropping)\n",
    "if 255 in test_final.columns:\n",
    "    test_final.drop(columns=[255], axis=1, inplace=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "test_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81dc01f3-9a20-4ab2-86b4-f69ccc9bc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the final file to csv\n",
    "test_final.to_csv('final_1D.csv',header=False,index=False)\n",
    "# Drop rows containing any NaN values\n",
    "test_final_clean = test_final.dropna()\n",
    "\n",
    "# Apply PCA or further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c00805-d763-41bf-92cc-d863bd585dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST DIMENSIONALITY REDUCTION EXPLAINED VARIANCE ON THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4f1c6d4-1626-4203-90b3-7915701f0056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [8.39422306e-02 4.66547019e-02 3.96706687e-02 3.11028346e-02\n",
      " 2.64309451e-02 2.44550659e-02 2.22748461e-02 2.11681070e-02\n",
      " 2.02428102e-02 1.80266251e-02 1.69531670e-02 1.57909831e-02\n",
      " 1.53378128e-02 1.47356995e-02 1.37994619e-02 1.36393928e-02\n",
      " 1.29824998e-02 1.25498295e-02 1.20341540e-02 1.15423771e-02\n",
      " 1.10955767e-02 1.06915285e-02 1.06041938e-02 1.04034531e-02\n",
      " 9.82664832e-03 9.54516540e-03 9.15587323e-03 8.83858059e-03\n",
      " 8.43311244e-03 8.30717959e-03 7.89500169e-03 7.70983323e-03\n",
      " 7.49110057e-03 7.29232846e-03 7.12388652e-03 7.04945459e-03\n",
      " 6.80121299e-03 6.49393661e-03 6.43632429e-03 6.14072518e-03\n",
      " 6.01833375e-03 5.80626359e-03 5.76755515e-03 5.67530288e-03\n",
      " 5.56315191e-03 5.45496914e-03 5.27295170e-03 5.14247746e-03\n",
      " 5.06599902e-03 4.87746887e-03 4.78506387e-03 4.63458939e-03\n",
      " 4.55236183e-03 4.44838071e-03 4.32679171e-03 4.22102959e-03\n",
      " 4.14894076e-03 4.10405038e-03 4.09696763e-03 3.89586138e-03\n",
      " 3.77741687e-03 3.74810193e-03 3.68420377e-03 3.64097141e-03\n",
      " 3.52649429e-03 3.45454090e-03 3.39368376e-03 3.38584410e-03\n",
      " 3.30367295e-03 3.20654997e-03 3.16505356e-03 3.12338961e-03\n",
      " 3.03268391e-03 3.02878720e-03 2.96499085e-03 2.94263302e-03\n",
      " 2.86372672e-03 2.80837880e-03 2.78227482e-03 2.71209908e-03\n",
      " 2.70075845e-03 2.63688711e-03 2.61655706e-03 2.55336097e-03\n",
      " 2.53570358e-03 2.48381600e-03 2.45826810e-03 2.42622819e-03\n",
      " 2.39841307e-03 2.33595176e-03 2.29133740e-03 2.24737927e-03\n",
      " 2.20611576e-03 2.15782009e-03 2.13501603e-03 2.11809473e-03\n",
      " 2.06662659e-03 2.03508014e-03 2.02042857e-03 1.98188128e-03\n",
      " 1.96230500e-03 1.93745481e-03 1.92593206e-03 1.85581849e-03\n",
      " 1.83504734e-03 1.82132134e-03 1.80446591e-03 1.79575050e-03\n",
      " 1.72899276e-03 1.70471820e-03 1.68655085e-03 1.66652360e-03\n",
      " 1.64027288e-03 1.63011381e-03 1.60941114e-03 1.56801281e-03\n",
      " 1.56290019e-03 1.53142048e-03 1.52317724e-03 1.48953806e-03\n",
      " 1.47399927e-03 1.44600868e-03 1.42632411e-03 1.41376821e-03\n",
      " 1.39318937e-03 1.38045030e-03 1.37442893e-03 1.33630809e-03\n",
      " 1.32759886e-03 1.29850293e-03 1.28094004e-03 1.25598785e-03\n",
      " 1.22925159e-03 1.21902226e-03 1.20691013e-03 1.18916010e-03\n",
      " 1.17716393e-03 1.16950483e-03 1.15253714e-03 1.14730075e-03\n",
      " 1.12779552e-03 1.11435441e-03 1.11251593e-03 1.10027897e-03\n",
      " 1.07989815e-03 1.07044256e-03 1.05569598e-03 1.03940234e-03\n",
      " 1.02721890e-03 9.97527851e-04 9.86023438e-04 9.77601260e-04\n",
      " 9.68294342e-04 9.56765652e-04 9.49389801e-04 9.35085085e-04\n",
      " 9.28807646e-04 9.13860926e-04 9.01545861e-04 8.95649386e-04\n",
      " 8.75500900e-04 8.73068248e-04 8.43994648e-04 8.37391865e-04\n",
      " 8.30376953e-04 8.22705204e-04 8.10496966e-04 7.96382990e-04\n",
      " 7.93208215e-04 7.87339816e-04 7.77933079e-04 7.61630032e-04\n",
      " 7.55496939e-04 7.48419093e-04 7.42622987e-04 7.29994630e-04\n",
      " 7.18347425e-04 7.03368303e-04 6.97463899e-04 6.86466525e-04\n",
      " 6.84795226e-04 6.70148230e-04 6.66238189e-04 6.51123577e-04\n",
      " 6.45157126e-04 6.38664482e-04 6.32162878e-04 6.15389526e-04\n",
      " 6.10263072e-04 6.05919481e-04 6.03382015e-04 5.90209271e-04\n",
      " 5.81549645e-04 5.75546907e-04 5.67421742e-04 5.56467157e-04\n",
      " 5.53407472e-04 5.47966628e-04 5.42213787e-04 5.36430254e-04\n",
      " 5.29605202e-04 5.25562504e-04 5.22338361e-04 5.17042145e-04\n",
      " 5.07749249e-04 5.00820107e-04 4.97913277e-04 4.89334827e-04\n",
      " 4.84530258e-04 4.71071554e-04 4.68972124e-04 4.64309790e-04\n",
      " 4.59851878e-04 4.53159240e-04 4.49836247e-04 4.38410969e-04\n",
      " 4.34233789e-04 4.32663286e-04 4.25231066e-04 4.20686897e-04\n",
      " 4.19342462e-04 4.11535079e-04 4.05294830e-04 4.02798765e-04\n",
      " 4.00509424e-04 3.95428583e-04 3.91052527e-04 3.86700714e-04\n",
      " 3.82813088e-04 3.76153866e-04 3.69779838e-04 3.66816449e-04\n",
      " 3.62688204e-04 3.59934616e-04 3.56008585e-04 3.53832019e-04\n",
      " 3.48240026e-04 3.43916145e-04 3.41090678e-04 3.32496682e-04\n",
      " 3.31599279e-04 3.28586397e-04 3.20532626e-04 3.17259718e-04\n",
      " 3.11497255e-04 3.09076312e-04 3.07853176e-04 3.03841748e-04\n",
      " 3.01464536e-04 2.97964755e-04 2.89304604e-04 2.86215609e-04\n",
      " 2.85711975e-04 2.80890460e-04 2.78892063e-04 2.78152946e-04\n",
      " 2.68917082e-04 2.66376209e-04 2.63245444e-04 2.60022251e-04\n",
      " 2.58098114e-04 2.55358904e-04 2.53890744e-04 2.51334677e-04\n",
      " 2.48150701e-04 2.47509713e-04 2.44659685e-04 2.40640675e-04\n",
      " 2.39224130e-04 2.34858216e-04 2.28900878e-04 2.25841244e-04\n",
      " 2.23625905e-04 2.21614424e-04 2.18104074e-04 2.17074948e-04\n",
      " 2.16804109e-04 2.11844791e-04 2.10052649e-04 2.07145583e-04\n",
      " 2.04074645e-04 2.01747315e-04 1.99831744e-04 1.97836347e-04\n",
      " 1.96502169e-04 1.93404466e-04 1.91928326e-04 1.88307619e-04\n",
      " 1.86148227e-04 1.83603680e-04 1.82229101e-04 1.81171225e-04\n",
      " 1.79641730e-04 1.77620008e-04 1.71620387e-04 1.70904908e-04\n",
      " 1.67402763e-04 1.64196780e-04 1.63074944e-04 1.62210998e-04\n",
      " 1.61852789e-04 1.58165407e-04 1.56447756e-04 1.53792021e-04\n",
      " 1.53157997e-04 1.51453316e-04 1.49955493e-04 1.49338286e-04\n",
      " 1.47122374e-04 1.43757441e-04 1.41909546e-04 1.40504788e-04\n",
      " 1.38122728e-04 1.37521934e-04 1.33563716e-04 1.32605889e-04\n",
      " 1.30091732e-04 1.28495325e-04 1.27611528e-04 1.26027265e-04\n",
      " 1.24082122e-04 1.22429357e-04 1.20892553e-04 1.20492264e-04\n",
      " 1.18548360e-04 1.17004353e-04 1.15515381e-04 1.13521419e-04\n",
      " 1.12703738e-04 1.11731982e-04 1.10550414e-04 1.08175157e-04\n",
      " 1.06720039e-04 1.05676511e-04 1.04611619e-04 1.02916839e-04\n",
      " 1.02602753e-04 1.00567553e-04 9.86685901e-05 9.72817373e-05\n",
      " 9.58241693e-05 9.49490028e-05 9.43585272e-05 9.33242366e-05\n",
      " 9.18288967e-05 9.03738933e-05 8.98511068e-05 8.83419355e-05\n",
      " 8.78747836e-05 8.69241444e-05 8.51990618e-05 8.39383059e-05\n",
      " 8.36934645e-05 8.30789418e-05 8.16266655e-05 8.02536153e-05\n",
      " 7.95899727e-05 7.78438797e-05 7.73431911e-05 7.63865468e-05\n",
      " 7.39179364e-05 7.32161488e-05 7.27500294e-05 7.14814945e-05\n",
      " 7.09462326e-05 6.95072288e-05 6.92805886e-05 6.88239567e-05\n",
      " 6.86670544e-05 6.70017361e-05 6.59152088e-05 6.47163895e-05\n",
      " 6.36060740e-05 6.34710942e-05 6.26725096e-05 6.14233069e-05\n",
      " 6.09397127e-05 6.02449780e-05 5.89108667e-05 5.84265081e-05\n",
      " 5.77177305e-05 5.66675949e-05 5.60746045e-05 5.56545561e-05\n",
      " 5.47540270e-05 5.32575881e-05 5.29041015e-05 5.23861495e-05\n",
      " 5.17803605e-05 5.15364415e-05 4.99809446e-05 4.91867733e-05\n",
      " 4.85723238e-05 4.74083201e-05 4.66382270e-05 4.56437915e-05\n",
      " 4.48428857e-05 4.40690630e-05 4.33451273e-05 4.27925999e-05]\n",
      "\n",
      " Total Variance Explained: 99.78\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.122380</td>\n",
       "      <td>-4.450834</td>\n",
       "      <td>6.060469</td>\n",
       "      <td>-2.537133</td>\n",
       "      <td>0.470744</td>\n",
       "      <td>-0.421343</td>\n",
       "      <td>-3.307106</td>\n",
       "      <td>2.110924</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>1.996462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>0.150393</td>\n",
       "      <td>-0.205803</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>-0.063077</td>\n",
       "      <td>0.019056</td>\n",
       "      <td>0.083759</td>\n",
       "      <td>0.067326</td>\n",
       "      <td>-0.053574</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.913497</td>\n",
       "      <td>4.907411</td>\n",
       "      <td>0.751201</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>-0.972549</td>\n",
       "      <td>1.822687</td>\n",
       "      <td>0.586439</td>\n",
       "      <td>1.269342</td>\n",
       "      <td>1.466850</td>\n",
       "      <td>-1.348596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040068</td>\n",
       "      <td>-0.016342</td>\n",
       "      <td>-0.055850</td>\n",
       "      <td>-0.088726</td>\n",
       "      <td>-0.115296</td>\n",
       "      <td>-0.130124</td>\n",
       "      <td>0.146551</td>\n",
       "      <td>0.029516</td>\n",
       "      <td>0.018298</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.397864</td>\n",
       "      <td>-1.112703</td>\n",
       "      <td>-0.865717</td>\n",
       "      <td>-0.976962</td>\n",
       "      <td>1.882616</td>\n",
       "      <td>3.339264</td>\n",
       "      <td>4.387539</td>\n",
       "      <td>0.292010</td>\n",
       "      <td>-2.147370</td>\n",
       "      <td>-4.560265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021609</td>\n",
       "      <td>-0.117020</td>\n",
       "      <td>0.078712</td>\n",
       "      <td>-0.044022</td>\n",
       "      <td>-0.148126</td>\n",
       "      <td>0.171856</td>\n",
       "      <td>-0.091587</td>\n",
       "      <td>0.164075</td>\n",
       "      <td>0.014224</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.241764</td>\n",
       "      <td>-1.606572</td>\n",
       "      <td>-0.209449</td>\n",
       "      <td>-2.707996</td>\n",
       "      <td>-3.232426</td>\n",
       "      <td>-4.149706</td>\n",
       "      <td>1.169299</td>\n",
       "      <td>-1.306792</td>\n",
       "      <td>-0.464859</td>\n",
       "      <td>-0.666980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260999</td>\n",
       "      <td>-0.067941</td>\n",
       "      <td>-0.083009</td>\n",
       "      <td>-0.051989</td>\n",
       "      <td>0.179367</td>\n",
       "      <td>-0.026153</td>\n",
       "      <td>-0.077580</td>\n",
       "      <td>-0.022059</td>\n",
       "      <td>0.223783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.329931</td>\n",
       "      <td>-1.294962</td>\n",
       "      <td>-1.650353</td>\n",
       "      <td>-3.887613</td>\n",
       "      <td>3.281349</td>\n",
       "      <td>5.731826</td>\n",
       "      <td>0.994176</td>\n",
       "      <td>-1.181468</td>\n",
       "      <td>1.869060</td>\n",
       "      <td>2.651752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017730</td>\n",
       "      <td>-0.143940</td>\n",
       "      <td>0.070195</td>\n",
       "      <td>0.078429</td>\n",
       "      <td>0.085700</td>\n",
       "      <td>-0.008124</td>\n",
       "      <td>-0.017254</td>\n",
       "      <td>-0.106624</td>\n",
       "      <td>-0.029322</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-6.306174</td>\n",
       "      <td>0.051323</td>\n",
       "      <td>-3.192195</td>\n",
       "      <td>-2.864543</td>\n",
       "      <td>2.691835</td>\n",
       "      <td>-1.797290</td>\n",
       "      <td>1.453815</td>\n",
       "      <td>0.908110</td>\n",
       "      <td>-0.401152</td>\n",
       "      <td>0.412460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.124405</td>\n",
       "      <td>-0.119051</td>\n",
       "      <td>-0.095905</td>\n",
       "      <td>0.123623</td>\n",
       "      <td>-0.047571</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.089644</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-2.117735</td>\n",
       "      <td>0.427470</td>\n",
       "      <td>4.346232</td>\n",
       "      <td>4.101731</td>\n",
       "      <td>1.728147</td>\n",
       "      <td>-2.703283</td>\n",
       "      <td>0.391755</td>\n",
       "      <td>3.100982</td>\n",
       "      <td>-2.585907</td>\n",
       "      <td>-2.933529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>-0.060947</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>-0.118641</td>\n",
       "      <td>0.187338</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>-0.053523</td>\n",
       "      <td>0.077431</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>5.741865</td>\n",
       "      <td>0.883400</td>\n",
       "      <td>-2.173462</td>\n",
       "      <td>2.001536</td>\n",
       "      <td>0.370380</td>\n",
       "      <td>-2.134055</td>\n",
       "      <td>-1.098237</td>\n",
       "      <td>-1.607278</td>\n",
       "      <td>-1.834999</td>\n",
       "      <td>-1.219355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165694</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.125975</td>\n",
       "      <td>-0.030640</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.027164</td>\n",
       "      <td>-0.069518</td>\n",
       "      <td>-0.031261</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>11.977949</td>\n",
       "      <td>0.390035</td>\n",
       "      <td>3.706434</td>\n",
       "      <td>1.805576</td>\n",
       "      <td>1.923991</td>\n",
       "      <td>-0.499650</td>\n",
       "      <td>-0.166997</td>\n",
       "      <td>1.007962</td>\n",
       "      <td>1.643489</td>\n",
       "      <td>1.482835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068380</td>\n",
       "      <td>-0.024244</td>\n",
       "      <td>0.006377</td>\n",
       "      <td>-0.131226</td>\n",
       "      <td>0.023073</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>-0.055134</td>\n",
       "      <td>-0.180007</td>\n",
       "      <td>-0.061851</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-0.608678</td>\n",
       "      <td>-4.404071</td>\n",
       "      <td>1.772526</td>\n",
       "      <td>1.329827</td>\n",
       "      <td>-2.197455</td>\n",
       "      <td>0.883641</td>\n",
       "      <td>-1.726994</td>\n",
       "      <td>-2.023422</td>\n",
       "      <td>1.554160</td>\n",
       "      <td>-0.834073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130385</td>\n",
       "      <td>-0.009624</td>\n",
       "      <td>-0.073858</td>\n",
       "      <td>-0.062841</td>\n",
       "      <td>-0.067914</td>\n",
       "      <td>-0.077155</td>\n",
       "      <td>-0.014367</td>\n",
       "      <td>-0.065080</td>\n",
       "      <td>-0.200299</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     3.122380 -4.450834  6.060469 -2.537133  0.470744 -0.421343 -3.307106   \n",
       "1    -0.913497  4.907411  0.751201  0.433470 -0.972549  1.822687  0.586439   \n",
       "2    -7.397864 -1.112703 -0.865717 -0.976962  1.882616  3.339264  4.387539   \n",
       "3    -3.241764 -1.606572 -0.209449 -2.707996 -3.232426 -4.149706  1.169299   \n",
       "4    -5.329931 -1.294962 -1.650353 -3.887613  3.281349  5.731826  0.994176   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "923  -6.306174  0.051323 -3.192195 -2.864543  2.691835 -1.797290  1.453815   \n",
       "924  -2.117735  0.427470  4.346232  4.101731  1.728147 -2.703283  0.391755   \n",
       "925   5.741865  0.883400 -2.173462  2.001536  0.370380 -2.134055 -1.098237   \n",
       "926  11.977949  0.390035  3.706434  1.805576  1.923991 -0.499650 -0.166997   \n",
       "927  -0.608678 -4.404071  1.772526  1.329827 -2.197455  0.883641 -1.726994   \n",
       "\n",
       "            7         8         9  ...       391       392       393  \\\n",
       "0    2.110924  0.625851  1.996462  ...  0.022786  0.150393 -0.205803   \n",
       "1    1.269342  1.466850 -1.348596  ...  0.040068 -0.016342 -0.055850   \n",
       "2    0.292010 -2.147370 -4.560265  ...  0.021609 -0.117020  0.078712   \n",
       "3   -1.306792 -0.464859 -0.666980  ... -0.260999 -0.067941 -0.083009   \n",
       "4   -1.181468  1.869060  2.651752  ... -0.017730 -0.143940  0.070195   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.908110 -0.401152  0.412460  ...  0.003779  0.124405 -0.119051   \n",
       "924  3.100982 -2.585907 -2.933529  ...  0.039473 -0.060947  0.013572   \n",
       "925 -1.607278 -1.834999 -1.219355  ...  0.165694  0.035790  0.003847   \n",
       "926  1.007962  1.643489  1.482835  ... -0.068380 -0.024244  0.006377   \n",
       "927 -2.023422  1.554160 -0.834073  ...  0.130385 -0.009624 -0.073858   \n",
       "\n",
       "          394       395       396       397       398       399  target  \n",
       "0    0.023661 -0.063077  0.019056  0.083759  0.067326 -0.053574       2  \n",
       "1   -0.088726 -0.115296 -0.130124  0.146551  0.029516  0.018298       2  \n",
       "2   -0.044022 -0.148126  0.171856 -0.091587  0.164075  0.014224       2  \n",
       "3   -0.051989  0.179367 -0.026153 -0.077580 -0.022059  0.223783       2  \n",
       "4    0.078429  0.085700 -0.008124 -0.017254 -0.106624 -0.029322       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.095905  0.123623 -0.047571  0.019471  0.089644  0.025950       3  \n",
       "924 -0.118641  0.187338  0.033646 -0.053523  0.077431 -0.001114       3  \n",
       "925  0.125975 -0.030640  0.037657  0.027164 -0.069518 -0.031261       3  \n",
       "926 -0.131226  0.023073 -0.026134 -0.055134 -0.180007 -0.061851       3  \n",
       "927 -0.062841 -0.067914 -0.077155 -0.014367 -0.065080 -0.200299       3  \n",
       "\n",
       "[928 rows x 401 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 400\n",
    "pca = PCA(n_components=400)\n",
    "x_pca = pca.fit_transform(test_final)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(result_df.iloc[:,-1], name='target')\n",
    "final_result_df = pd.concat([pca_df, target], axis=1)\n",
    "final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9ffeb97-67fc-4e23-b2f9-0196ef01f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to dimensionally reduced csv file\n",
    "final_result_df.to_csv(\"pca_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c21e74e-4553-4e22-9065-5616e9f673d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCA_ECG.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "#save the PCA model\n",
    "joblib_file='PCA_ECG.pkl'\n",
    "joblib.dump(pca,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543b531-3a8a-4c02-859a-809c76e4b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRYING DIFFERENT ML MODELS ON THE ALL 12 LEADS COMBINED FILE WITHOUT DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5a3324a-dc0e-4425-b2f4-968ecde47e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8360215053763441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.69      0.79       105\n",
      "           1       0.93      1.00      0.96        94\n",
      "           2       0.78      0.89      0.83       112\n",
      "           3       0.69      0.74      0.71        61\n",
      "\n",
      "    accuracy                           0.84       372\n",
      "   macro avg       0.83      0.83      0.82       372\n",
      "weighted avg       0.84      0.84      0.83       372\n",
      "\n",
      "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
    "k_range = list(range(1, 30))\n",
    "parameters = dict(knn__n_neighbors=k_range)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "Knn_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61b5040d-a1f4-4b67-8f22-ac937177a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('lr', LogisticRegression())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "#parameters for gridsearchcv\n",
    "c_space = np.logspace(-4, 4, 10)\n",
    "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#call GridSearchCV and set crossvalscore to 2\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "LR_Accuracy = cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3a2e867-5753-41c5-a35c-2f082593ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8091397849462365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.65      0.74       105\n",
      "           1       0.90      1.00      0.95        94\n",
      "           2       0.78      0.86      0.82       112\n",
      "           3       0.66      0.70      0.68        61\n",
      "\n",
      "    accuracy                           0.81       372\n",
      "   macro avg       0.80      0.80      0.80       372\n",
      "weighted avg       0.81      0.81      0.81       372\n",
      "\n",
      "Tuned Model Parameters: {'lr__C': 0.3593813663804626, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b3db525-b1f6-45da-afa8-3ef5e8090f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.896551724137931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86       119\n",
      "           1       1.00      1.00      1.00       125\n",
      "           2       0.84      0.93      0.88       140\n",
      "           3       0.96      0.68      0.79        80\n",
      "\n",
      "    accuracy                           0.90       464\n",
      "   macro avg       0.91      0.88      0.89       464\n",
      "weighted avg       0.90      0.90      0.89       464\n",
      "\n",
      "Tuned Model Parameters: {'SVM__C': 10, 'SVM__gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41ef7e51-a263-498c-a19d-f85eaace7e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f546e485-8131-4e03-84b3-683d847306f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52e3b24d-2164-4c62-8bce-459c5112c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8599137931034483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       119\n",
      "           1       0.97      1.00      0.98       125\n",
      "           2       0.82      0.91      0.86       140\n",
      "           3       0.83      0.68      0.74        80\n",
      "\n",
      "    accuracy                           0.86       464\n",
      "   macro avg       0.86      0.84      0.85       464\n",
      "weighted avg       0.86      0.86      0.86       464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519eb3e-57a8-4b7a-a589-c9ddbbe719a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVING A VERY BASIC ML MODEL AND USING IT ON REALTIME PIPELINE TO CHECK WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97dce20c-d5e0-4010-9e71-02f2479af51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_model_test.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='knn_model_test.pkl'\n",
    "joblib.dump(knn,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "116db432-a045-4842-9638-b5978be794c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model_test.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#input\n",
    "X = pd.read_csv('final_1D.csv',header=None)\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "svm=SVC(C=10,gamma=0.01)\n",
    "\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='svm_model_test.pkl'\n",
    "joblib.dump(svm,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a63afc9-686f-4860-8630-5c5824096030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "from sklearn import linear_model, tree, ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0db8f222-54aa-45db-8761-b7c5387a1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "X = final_result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66130329-1fd1-4298-a848-9a42694b3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking of ML Models\n",
    "eclf = VotingClassifier(estimators=[ \n",
    "    ('SVM', SVC(probability=True)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('rf', ensemble.RandomForestClassifier()),\n",
    "    ('bayes',GaussianNB()),\n",
    "    ('logistic',LogisticRegression()),\n",
    "    ], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94716c5c-dc35-41a1-9612-10c933b6aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM__C': 10, 'SVM__gamma': 0.1, 'knn__n_neighbors': 5, 'rf__n_estimators': 300}\n",
      "Accuracy: 0.931899641577061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96        80\n",
      "           1       1.00      1.00      1.00        72\n",
      "           2       0.88      0.92      0.90        79\n",
      "           3       0.88      0.79      0.84        48\n",
      "\n",
      "    accuracy                           0.93       279\n",
      "   macro avg       0.93      0.92      0.92       279\n",
      "weighted avg       0.93      0.93      0.93       279\n",
      "\n",
      "{'SVM__C': 10, 'SVM__gamma': 0.1, 'knn__n_neighbors': 5, 'rf__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using gridSearch\n",
    "params = {'SVM__C':[1, 10, 100],\n",
    "          'SVM__gamma':[0.1, 0.01],\n",
    "          'knn__n_neighbors': [1,3,5],\n",
    "          'rf__n_estimators':[300, 400],\n",
    "          }\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "voting_clf = grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "Voting_Accuracy=voting_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(Voting_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(voting_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb090a9-9825-4c9f-aa3f-5c73ebf9471f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
